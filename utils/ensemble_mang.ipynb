{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('openmmlab': conda)"
  },
  "interpreter": {
   "hash": "4eaf873e9632301d129c27e1df38bb41050bb400cabd97461e86c0cc3cb82c3f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting easydict\n",
      "  Using cached easydict-1.9-py3-none-any.whl\n",
      "Installing collected packages: easydict\n",
      "Successfully installed easydict-1.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/envs/openmmlab/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import cv2\n",
    "import torchvision\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import argparse\n",
    "import mmcv\n",
    "from mmcv import Config\n",
    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
    "                            replace_ImageToTensor)\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import single_gpu_test\n",
    "from mmcv.runner import load_checkpoint\n",
    "import os\n",
    "from mmcv.parallel import MMDataParallel\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dir = 'sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_704-1024_36_coco'\n",
    "# exp_name = 'sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_704-1024_36_coco.py'\n",
    "# cfg_path = f'../mmdetection/configs/{exp_dir}/{exp_name}'\n",
    "# print(cfg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../work_dirs/sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_704-1024_36_coco\n{'type': 'CocoDataset', 'ann_file': '../dataset2/test.json', 'img_prefix': '../dataset2/', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (512, 512), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'Pad', 'size_divisor': 32}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}], 'classes': ('General trash', 'Paper', 'Paper pack', 'Metal', 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery', 'Clothing'), 'test_mode': True}\nloading annotations into memory...\nDone (t=0.01s)\ncreating index...\nindex created!\n"
     ]
    }
   ],
   "source": [
    "def load_dataloader(exp_dir,exp_name,checkpoint_path):\n",
    "    cfg_path = f'../mmdetection/configs/{exp_dir}/{exp_name}'\n",
    "    config = cfg_path\n",
    "    \n",
    "    args = EasyDict({\n",
    "        'config' : config,\n",
    "        'checkpoint': checkpoint_path,\n",
    "        'score_thr' :0.001,\n",
    "        'iou_thr': 0.5, \n",
    "        'work_dir': None,\n",
    "        'seed': 42\n",
    "    })\n",
    "\n",
    "    cfg = Config.fromfile(args.config)\n",
    "    seed = args.seed\n",
    "    cfg.gpu_ids = [1]\n",
    "\n",
    "    cfg.work_dir = osp.join('../work_dirs',osp.splitext(osp.basename(args.config))[0])\n",
    "    print(cfg.work_dir)\n",
    "    cfg.data.test.test_mode = True\n",
    "    \n",
    "\n",
    "    cfg.optimizer_config.grad_clip = dict(max_norm=35, norm_type=2)\n",
    "    cfg.model.train_cfg = None\n",
    "    cfg.model.test_cfg.rcnn.score_thr = args.score_thr\n",
    "    cfg.model.test_cfg.rcnn.iou_threshold = args.iou_thr\n",
    "    \n",
    "    cfg.data.test['ann_file'] = os.path.join('../',cfg.data.test['ann_file'])\n",
    "    cfg.data.test['img_prefix'] = os.path.join('../',cfg.data.test['img_prefix'])\n",
    "    print(cfg.data.test)\n",
    "    \n",
    "    dataset = build_dataset(cfg.data.test)\n",
    "    data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        samples_per_gpu=1,\n",
    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "data_loader = load_dataloader(exp_dir='sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_704-1024_36_coco',\n",
    "        exp_name='sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_704-1024_36_coco.py',\n",
    "            checkpoint_path='best_bbox_mAP_50_epoch_22'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "CocoDataset Test dataset with number of images 4871, and instance counts: \n",
       "+-------------------+-------+---------------+-------+-----------------+-------+-------------+-------+--------------+-------+\n",
       "| category          | count | category      | count | category        | count | category    | count | category     | count |\n",
       "+-------------------+-------+---------------+-------+-----------------+-------+-------------+-------+--------------+-------+\n",
       "| 0 [General trash] | 0     | 1 [Paper]     | 0     | 2 [Paper pack]  | 0     | 3 [Metal]   | 0     | 4 [Glass]    | 0     |\n",
       "| 5 [Plastic]       | 0     | 6 [Styrofoam] | 0     | 7 [Plastic bag] | 0     | 8 [Battery] | 0     | 9 [Clothing] | 0     |\n",
       "|                   |       |               |       |                 |       |             |       |              |       |\n",
       "| -1 background     | 4871  |               |       |                 |       |             |       |              |       |\n",
       "+-------------------+-------+---------------+-------+-----------------+-------+-------------+-------+--------------+-------+"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "data_loader[0].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'img_metas': [DataContainer({'filename': '../dataset2/test/0000.jpg', 'ori_filename': 'test/0000.jpg', 'ori_shape': (1024, 1024, 3), 'img_shape': (512, 512, 3), 'pad_shape': (512, 512, 3), 'scale_factor': array([0.5, 0.5, 0.5, 0.5], dtype=float32), 'flip': False, 'flip_direction': None, 'img_norm_cfg': {'mean': array([123.675, 116.28 , 103.53 ], dtype=float32), 'std': array([58.395, 57.12 , 57.375], dtype=float32), 'to_rgb': True}})], 'img': [tensor([[[ 0.4337,  0.4337,  0.4337,  ...,  0.5536,  0.1254,  0.7248],\n         [ 0.4337,  0.4337,  0.4337,  ...,  0.6906,  0.5193,  1.1187],\n         [ 0.4337,  0.4337,  0.4337,  ...,  1.0502,  0.3823,  0.7591],\n         ...,\n         [-0.8164, -0.8849, -0.9020,  ...,  0.7419,  0.4337,  0.0741],\n         [-0.8678, -0.9534, -0.9192,  ...,  0.8961,  0.6221,  0.3481],\n         [-0.9705, -0.9877, -0.8678,  ...,  0.5707,  0.7933,  0.5878]],\n\n        [[ 0.1001,  0.1001,  0.1001,  ...,  0.6429,  0.2052,  0.8179],\n         [ 0.1001,  0.1001,  0.1001,  ...,  0.7829,  0.6078,  1.2206],\n         [ 0.1001,  0.1001,  0.1001,  ...,  1.1506,  0.4678,  0.8529],\n         ...,\n         [-0.7227, -0.7927, -0.8102,  ...,  0.9055,  0.5903,  0.2227],\n         [-0.7752, -0.8627, -0.8277,  ...,  1.0630,  0.7829,  0.5028],\n         [-0.8803, -0.8978, -0.7752,  ...,  0.7304,  0.9580,  0.7479]],\n\n        [[-0.2010, -0.2010, -0.2010,  ...,  0.7751,  0.3393,  0.9494],\n         [-0.2010, -0.2010, -0.2010,  ...,  0.9145,  0.7402,  1.3502],\n         [-0.2010, -0.2010, -0.2010,  ...,  1.2805,  0.6008,  0.9842],\n         ...,\n         [-0.5670, -0.6367, -0.6541,  ...,  1.0191,  0.7054,  0.3393],\n         [-0.6193, -0.7064, -0.6715,  ...,  1.1411,  0.8622,  0.5834],\n         [-0.7238, -0.7413, -0.6193,  ...,  0.8099,  1.0365,  0.8274]]])]}\n[tensor([[[ 0.4337,  0.4337,  0.4337,  ...,  0.5536,  0.1254,  0.7248],\n         [ 0.4337,  0.4337,  0.4337,  ...,  0.6906,  0.5193,  1.1187],\n         [ 0.4337,  0.4337,  0.4337,  ...,  1.0502,  0.3823,  0.7591],\n         ...,\n         [-0.8164, -0.8849, -0.9020,  ...,  0.7419,  0.4337,  0.0741],\n         [-0.8678, -0.9534, -0.9192,  ...,  0.8961,  0.6221,  0.3481],\n         [-0.9705, -0.9877, -0.8678,  ...,  0.5707,  0.7933,  0.5878]],\n\n        [[ 0.1001,  0.1001,  0.1001,  ...,  0.6429,  0.2052,  0.8179],\n         [ 0.1001,  0.1001,  0.1001,  ...,  0.7829,  0.6078,  1.2206],\n         [ 0.1001,  0.1001,  0.1001,  ...,  1.1506,  0.4678,  0.8529],\n         ...,\n         [-0.7227, -0.7927, -0.8102,  ...,  0.9055,  0.5903,  0.2227],\n         [-0.7752, -0.8627, -0.8277,  ...,  1.0630,  0.7829,  0.5028],\n         [-0.8803, -0.8978, -0.7752,  ...,  0.7304,  0.9580,  0.7479]],\n\n        [[-0.2010, -0.2010, -0.2010,  ...,  0.7751,  0.3393,  0.9494],\n         [-0.2010, -0.2010, -0.2010,  ...,  0.9145,  0.7402,  1.3502],\n         [-0.2010, -0.2010, -0.2010,  ...,  1.2805,  0.6008,  0.9842],\n         ...,\n         [-0.5670, -0.6367, -0.6541,  ...,  1.0191,  0.7054,  0.3393],\n         [-0.6193, -0.7064, -0.6715,  ...,  1.1411,  0.8622,  0.5834],\n         [-0.7238, -0.7413, -0.6193,  ...,  0.8099,  1.0365,  0.8274]]])]\n"
     ]
    }
   ],
   "source": [
    "for i,data in enumerate(data_loader):\n",
    "    # print(data.dataset)\n",
    "    for d in data.dataset:\n",
    "        print(d)\n",
    "        print(d[])\n",
    "        print(d['img'])\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../work_dirs/sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_704-1024_36_coco\n",
      "{'type': 'CocoDataset', 'ann_file': '../dataset2/test.json', 'img_prefix': 'dataset2/', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (512, 512), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'Pad', 'size_divisor': 32}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}], 'classes': ('General trash', 'Paper', 'Paper pack', 'Metal', 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery', 'Clothing'), 'test_mode': True}\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "epoch: best_bbox_mAP_50_epoch_22\n",
      "load checkpoint from local path: ../work_dirs/sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_704-1024_36_coco/best_bbox_mAP_50_epoch_22.pth\n",
      "====================\n",
      "model load finished\n",
      "\n",
      "../work_dirs/cascadeRCNN_r50_fpn\n",
      "{'type': 'CocoDataset', 'ann_file': '../dataset/test.json', 'img_prefix': 'dataset/', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (1024, 1024), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'Pad', 'size_divisor': 32}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}], 'classes': ('General trash', 'Paper', 'Paper pack', 'Metal', 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery', 'Clothing'), 'test_mode': True}\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "epoch: best_bbox_mAP_50_epoch_23\n",
      "load checkpoint from local path: ../work_dirs/cascadeRCNN_r50_fpn/best_bbox_mAP_50_epoch_23.pth\n",
      "====================\n",
      "model load finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "\n",
    "def load_net(exp_dir,exp_name,checkpoint_path):\n",
    "    cfg_path = f'../mmdetection/configs/{exp_dir}/{exp_name}'\n",
    "    config = cfg_path\n",
    "    \n",
    "    args = EasyDict({\n",
    "        'config' : config,\n",
    "        'checkpoint': checkpoint_path,\n",
    "        'score_thr' :0.001,\n",
    "        'iou_thr': 0.5, \n",
    "        'work_dir': None,\n",
    "        'seed': 42\n",
    "    })\n",
    "\n",
    "    cfg = Config.fromfile(args.config)\n",
    "    seed = args.seed\n",
    "    cfg.gpu_ids = [1]\n",
    "\n",
    "    cfg.work_dir = osp.join('../work_dirs',osp.splitext(osp.basename(args.config))[0])\n",
    "    print(cfg.work_dir)\n",
    "    cfg.data.test.test_mode = True\n",
    "    \n",
    "\n",
    "    cfg.optimizer_config.grad_clip = dict(max_norm=35, norm_type=2)\n",
    "    cfg.model.train_cfg = None\n",
    "    cfg.model.test_cfg.rcnn.score_thr = args.score_thr\n",
    "    cfg.model.test_cfg.rcnn.iou_threshold = args.iou_thr\n",
    "    \n",
    "    cfg.data.test['ann_file'] = os.path.join('../',cfg.data.test['ann_file'])\n",
    "    print(cfg.data.test)\n",
    "    \n",
    "    dataset = build_dataset(cfg.data.test)\n",
    "    data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        samples_per_gpu=1,\n",
    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "\n",
    "    epoch = args.checkpoint\n",
    "    print(f'epoch: {epoch}')\n",
    "    # checkpoint path\n",
    "    checkpoint_path = os.path.join(cfg.work_dir, f'{epoch}.pth')\n",
    "\n",
    "    model = build_detector(cfg.model, test_cfg=cfg.get(\n",
    "        'test_cfg'))  # build detector\n",
    "    checkpoint = load_checkpoint(\n",
    "        model, checkpoint_path, map_location='cpu')  # ckpt load\n",
    "    \n",
    "    model = MMDataParallel(model.cuda(), device_ids=[0])\n",
    "    \n",
    "    num_classes = 10  \n",
    "    model.eval()\n",
    "    gc.collect()\n",
    "    print('====================')\n",
    "    print('model load finished\\n')\n",
    "    return model\n",
    "\n",
    "models = [\n",
    "    load_net(exp_dir='sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_704-1024_36_coco',\n",
    "        exp_name='sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_704-1024_36_coco.py',\n",
    "            checkpoint_path='best_bbox_mAP_50_epoch_22'),\n",
    "    load_net(exp_dir='exp1',\n",
    "            exp_name='cascadeRCNN_r50_fpn.py',\n",
    "            checkpoint_path='best_bbox_mAP_50_epoch_23'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting ensemble-boxes\n",
      "  Downloading ensemble_boxes-1.0.8-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/openmmlab/lib/python3.7/site-packages (from ensemble-boxes) (1.3.4)\n",
      "Collecting numba\n",
      "  Downloading numba-0.55.1-1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 46.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/openmmlab/lib/python3.7/site-packages (from ensemble-boxes) (1.21.2)\n",
      "Collecting llvmlite<0.39,>=0.38.0rc1\n",
      "  Downloading llvmlite-0.38.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 53.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/envs/openmmlab/lib/python3.7/site-packages (from numba->ensemble-boxes) (58.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/openmmlab/lib/python3.7/site-packages (from pandas->ensemble-boxes) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/envs/openmmlab/lib/python3.7/site-packages (from pandas->ensemble-boxes) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/openmmlab/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->ensemble-boxes) (1.16.0)\n",
      "Installing collected packages: llvmlite, numba, ensemble-boxes\n",
      "Successfully installed ensemble-boxes-1.0.8 llvmlite-0.38.0 numba-0.55.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install ensemble-boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble_boxes import *\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "def make_ensemble_predictions(images):\n",
    "    images = list(image.to(device) for image in images)    \n",
    "    result = []\n",
    "    for net in models:\n",
    "        outputs = net(images)\n",
    "        result.append(outputs)\n",
    "    return result\n",
    "\n",
    "def run_wbf(predictions, image_index, image_size=512, iou_thr=0.55, skip_box_thr=0.7, weights=None):\n",
    "    boxes = [prediction[image_index]['boxes'].data.cpu().numpy()/(image_size-1) for prediction in predictions]\n",
    "    scores = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\n",
    "    labels = [np.ones(prediction[image_index]['scores'].shape[0]) for prediction in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16237/648320302.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for j, (images, image_ids) in enumerate(data_loader):\n",
    "    if j > 0:\n",
    "        break\n",
    "predictions = make_ensemble_predictions(images)\n",
    "\n",
    "i = 1\n",
    "sample = images[i].permute(1,2,0).cpu().numpy()\n",
    "boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "boxes = boxes.astype(np.int32).clip(min=0, max=511)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(sample,\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2], box[3]),\n",
    "                  (220, 0, 0), 2)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22574/1776016798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/openmmlab/lib/python3.7/site-packages/mmdet/datasets/custom.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_test_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_train_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/openmmlab/lib/python3.7/site-packages/mmdet/datasets/custom.py\u001b[0m in \u001b[0;36mprepare_test_img\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \"\"\"\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mimg_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "data = data_loader[0].dataset\n",
    "data['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WBF 적용되지 않은 결과\n",
    "\n",
    "fig, axes = plt.subplots(2, 5)\n",
    "fig.set_size_inches(15, 8)\n",
    "color = [(0,0,255),(255,0,255)]\n",
    "\n",
    "\n",
    "\n",
    "for i, (images, image_ids) in enumerate(test_loader):\n",
    "    predictions = make_ensemble_predictions(images)\n",
    "\n",
    "    img = cv2.imread(f\"/kaggle/input/global-wheat-detection/test/{image_ids[0]}.jpg\", cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    for folds in range(len(predictions)):\n",
    "        for index in range(len(predictions[folds][0]['boxes'])):\n",
    "            \n",
    "            # bbox 값의 범위를 이미지 사이즈(1024)에 맞춰준다.\n",
    "            bbox = (predictions[folds][0]['boxes'][index]*2)\n",
    "            bbox = bbox.data.cpu().numpy().astype(np.int32).clip(min=0, max=1023)\n",
    "            xmin, ymin, xmax, ymax = int(float(bbox[0])), int(float(bbox[1])), int(float(bbox[2])), int(float(bbox[3]))\n",
    "            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color[folds], 3)\n",
    "    axes[i//5, i%5].imshow(img)\n",
    "    axes[i//5, i%5].set_title(image_ids[0])\n",
    "    axes[i//5, i%5].axis('off')"
   ]
  }
 ]
}